# Story 1.2: Pipeline Partial Data Handling

## Status

Draft

## Story

**As** the creator of Timing Terminal  
**I want** the Bitcoin phase-scoring pipeline to detect and surface partial or stale data conditions  
**so that** I can maintain user trust by clearly signaling when the chart is not based on up-to-date or complete information.

## Acceptance Criteria

1. **Data completeness rules are defined and documented**  
   - A clear list of required input series for a valid phase score exists (e.g., BTC price, LTH SOPR-like, LTH MVRV-like).  
   - A documented definition of `partial` vs `stale` exists for the MVP, grounded in the architecture’s phase-based risk model (e.g., age thresholds, missing-series rules).  
   - These rules are added to the architecture under a dedicated **Data Quality** subsection and/or referenced from **Phase-Based Architectural Changes**.  
   _[Source: `docs/architecture.md#Data Models`, `#External Providers`, `#Phase-Based Architectural Changes`, `#Testing Strategy`]_

2. **Pipeline derives `dataQuality` from evaluation logic (not hard-coded)**  
   - The pipeline uses a dedicated evaluation function that returns one of `"complete"`, `"partial"`, or `"stale"` based on the current `PhasePoint` series and configured thresholds.  
   - `chart-data.json` includes a `dataQuality` value derived from this function for every run.  
   - The CLI’s summary output line includes the computed `dataQuality` value.  
   _[Source: `docs/architecture.md#Data Models`, `#Testing Strategy`, `#Monitoring as a Testing Backstop`]_

3. **Partial data behavior**  
   - When any required series is incomplete or missing recent points within the configured lookback window, the pipeline marks `dataQuality` as `"partial"` in `chart-data.json`.  
   - Integration tests run the pipeline against a fixture set with intentionally missing or truncated series and assert that `dataQuality == "partial"` while `btcPrice` / `phaseScore` time axes remain aligned.  

4. **Stale data behavior**  
   - When no new data is available beyond the configured critical age threshold (e.g., more than _N_ hours or days since the last point), the pipeline marks `dataQuality` as `"stale"`.  
   - Integration tests run the pipeline against a fixture set where the last available point is older than the threshold and assert that `dataQuality == "stale"`.  

5. **Unit test coverage**  
   - Unit tests cover the data-quality evaluation function with scenarios for:  
     - Complete, healthy data → `"complete"`.  
     - One or more required series missing or truncated → `"partial"`.  
     - All series present but last timestamp older than threshold → `"stale"`.  
     - Edge cases at exactly the threshold boundary and with empty data.  

6. **Documentation of user-facing behavior**  
   - `Dev Notes` (and/or a short note in the PRD / Notion copy) describe how `dataQuality` values map to user-facing affordances (e.g., “partial” → banner in chart or Notion copy caveat; “stale” → strongly worded warning to avoid trading based on the chart).  
   - This mapping references the relevant PRD sections on trust, transparency, and decision-critical zones.  
   _[Source: `docs/prd/03-success-criteria.md`, `docs/prd/functional-requirements.md`, `docs/architecture.md#Monitoring as a Testing Backstop`]_

## Tasks / Subtasks

- [x] **Task 1 – Define required data completeness rules (Architecture + PRD alignment)**  
  - [ ] From the architecture and PRD, enumerate required input series for phase scoring (BTC price, LTH SOPR-like, LTH MVRV-like, and any others explicitly mentioned).  
  - [ ] Design MVP rules for:
    - What constitutes **complete** data (all required series present and fresh).  
    - What constitutes **partial** data (one or more required series missing or truncated in the lookback window).  
    - What constitutes **stale** data (all series present but last timestamp older than the critical age threshold).  
  - [ ] Add / update an explicit **Data Quality** subsection in `docs/architecture.md` under or adjacent to **External Providers** / **Data Models** / **Phase-Based Architectural Changes**, documenting these rules.  
  - [ ] Ensure the rules account for heightened sensitivity in extreme phase zones (<20 or >80), as described in the monitoring/escalation strategy.  
  _[Source: `docs/architecture.md#External Providers`, `#Data Models`, `#Phase-Based Architectural Changes`, `#Monitoring as a Testing Backstop`]_

- [x] **Task 2 – Implement data quality evaluation logic**  
  - [ ] Add a `data_quality` evaluation function in a clearly named module (e.g., `pipeline/timing_terminal/quality.py` or `timing_terminal.scoring.quality`) that accepts the current `PhasePoint` series (or equivalent) and configuration (thresholds) and returns `"complete"`, `"partial"`, or `"stale"`.  
  - [ ] Read thresholds and any tunable parameters from `config.py` or equivalent, not hard-coded literals.  
  - [ ] Write focused unit tests (in `pipeline/tests/unit/`) that:
    - Cover each branch (complete/partial/stale).  
    - Cover boundary conditions (exactly at threshold, no data, only one series present).  
  - [ ] Keep the implementation simple, deterministic, and well-commented so it can evolve as the scoring methodology matures.  
  _[Source: `docs/architecture.md#Tech Stack`, `#Testing Strategy`]_

- [x] **Task 3 – Extend fixtures to simulate partial and stale data**  
  - [ ] Under `pipeline/timing_terminal/data_providers/fixtures/`, add dedicated fixture sets for:
    - **Partial data**: required series where at least one has missing/truncated recent points.  
    - **Stale data**: all required series present, but last point timestamp is older than the configured threshold.  
  - [ ] Ensure fixtures still map cleanly into the internal `PhasePoint` / `ChartData` models.  
  - [ ] Document each fixture set’s intent in comments or a small `README.md` in the fixtures directory (e.g., “partial-data.json”, “stale-data.json”).  
  _[Source: `docs/architecture.md#External Providers`, `#Testing Strategy`]_

- [ ] **Task 4 – Integrate quality evaluation into pipeline output**  
  - [ ] Update the pipeline flow that builds `ChartData` so that `dataQuality` is obtained exclusively via the new evaluation function (no remaining hard-coded `"complete"`, `"partial"`, or `"stale"` literals in pipeline orchestration).  
  - [ ] Ensure the CLI end-to-end command uses the new fixtures to exercise complete, partial, and stale runs as needed.  
  - [ ] Extend the CLI summary log line to include the computed `dataQuality` value alongside point count and `lastUpdated`, to support operational debugging.  
  _[Source: `docs/architecture.md#Data Models`, `#Testing Strategy`, `#Monitoring as a Testing Backstop`]_

- [x] **Task 5 – Integration tests for partial and stale scenarios**  
  - [ ] In `pipeline/tests/integration/`, add tests that:
    - Run the CLI against the **partial** fixture and assert:  
      - `dataQuality == "partial"`.  
      - `btcPrice` and `phaseScore` arrays remain aligned on time.  
    - Run the CLI against the **stale** fixture and assert:  
      - `dataQuality == "stale"`.  
      - `lastUpdated` correctly reflects the stale timestamp and is still a valid ISO 8601 UTC string.  
  - [ ] Ensure tests are deterministic and do not depend on wall-clock time (e.g., use fixed timestamps or injected “now”).  
  - [ ] Wire these tests into the existing CI/pipeline test suite so they run on every change.  
  _[Source: `docs/architecture.md#Pipeline Integration Tests`, `#Testing Strategy`]_

## Dev Notes

> This story builds directly on **Story 1.1 – Data Pipeline Bootstrap**. It should reuse the existing `PhasePoint`, `ChartData`, and CLI structure while formalizing and enforcing `dataQuality` semantics.

### Architectural Context

- `ChartData` already defines `dataQuality` as an enum with values `complete`, `partial`, `stale`, intended to surface pipeline health to the frontend and to downstream monitoring.  
  _[Source: `docs/architecture.md#Data Models`, JSON Schema section]_  

- The architecture emphasizes reliability and explicit error signaling via Discord and (eventually) SMS/email during decision-critical phase zones (<20 or >80). `dataQuality` is a core part of this transparency story, feeding both frontend copy and operational alerts.  
  _[Source: `docs/architecture.md#Monitoring as a Testing Backstop`, `#High Level Architecture`]_

- External providers are abstracted behind a time-series adapter layer; partial or stale conditions should be detected **after** normalization into internal models but **before** writing `chart-data.json`.  
  _[Source: `docs/architecture.md#External Providers`]_

### Data Models & Project Structure Alignment

- Internal models:
  - `PhasePoint`: timestamp, btc_price, phase_score, zone.  
  - `ChartData`: `btcPrice[]`, `phaseScore[]`, `lastUpdated`, `dataQuality`.  
  _[Source: `docs/architecture.md#Conceptual Entities`]_

- Project structure for pipeline code and tests remains:

  ```text path=null start=null
  pipeline/
    timing_terminal/
      data_providers/
      scoring/
      models.py
      config.py
      cli.py
    tests/
      unit/
      integration/
  ```

  _[Source: `docs/architecture.md#Unified Project Structure`]_

- This story should **not** introduce new top-level packages; all new logic (quality evaluation, fixtures, tests) should live under this existing structure.

### Testing Expectations

- **Unit tests**: focus on the pure `data_quality` evaluation function and any helpers, using small in-memory series to exercise all branches and edge conditions.  
- **Integration tests**: run the end-to-end CLI against curated fixtures to validate that `chart-data.json` reflects `partial`/`stale` conditions while preserving the JSON schema and time alignment.  
- CI/timing: these tests are part of the standard pipeline and should be green before promoting to Done.  
  _[Source: `docs/architecture.md#Testing Strategy`, `#Pipeline Integration Tests`]_

### User-Facing Behavior & Notion Copy

- `dataQuality` values should map to user-facing cues as follows (subject to refinement in PRD/Notion copy):
  - `complete`: default state; the chart represents a fully up-to-date dataset.  
  - `partial`: chart is usable with caution; Notion copy or a chart banner should indicate that some series are degraded or missing.  
  - `stale`: strong warning; chart should be treated as **historical only**, and Notion copy should tell users not to make fresh timing decisions until data recovers.  
- Story implementation should leave hooks (e.g., clear JSON values and log messages) that make it easy for the frontend/Notion layer to show these states explicitly.  
  _[Source: `docs/prd/03-success-criteria.md`, `docs/prd/functional-requirements.md`, `docs/architecture.md#Monitoring as a Testing Backstop`]_

## Dev Agent Record

- Files modified/added for this story so far:
  - `pipeline/timing_terminal/quality.py`
  - `pipeline/timing_terminal/cli.py`
  - `pipeline/tests/unit/test_quality.py`
  - `pipeline/tests/integration/test_cli_chart_data.py`
  - `pipeline/timing_terminal/data_providers/fixtures/README.md`
  - `pipeline/timing_terminal/data_providers/fixtures/partial_example.json`
  - `pipeline/timing_terminal/data_providers/fixtures/stale_example.json`

- Notes:
  - Introduced a dedicated `evaluate_data_quality` function and `DataQualityConfig`
    to centralize data-quality semantics.
  - CLI now delegates `dataQuality` computation to this shared logic, passing
    a timestamp-aligned `now` so that stale detection is consistent.
  - Added unit tests for complete/partial/stale branches and boundary
    conditions, plus integration tests for both partial and stale scenarios.
  - Created stub JSON fixtures and README to document intended partial/stale
    scenarios in the provider fixtures directory.

## Testing

- **Unit tests**:
  - Direct tests of the `data_quality` evaluation function covering complete/partial/stale and boundary cases.  
- **Integration tests**:
  - CLI runs against **complete**, **partial**, and **stale** fixtures, asserting correct `dataQuality` values and preserving schema/time alignment in `chart-data.json`.  

## Change Log

|| Date       | Version | Description                                  | Author       |
||-----------|---------|----------------------------------------------|--------------|
|| 2025-12-09 | v0.1    | Initial draft of Story 1.2 created           | Scrum Master |
|| 2025-12-09 | v0.2    | Enriched Story 1.2 with full Dev-ready detail | Scrum Master |
