# Story 1.2: Pipeline Partial Data Handling

## Status

Draft

## Story

**As** the creator of Timing Terminal  
**I want** the Bitcoin phase-scoring pipeline to detect and surface partial or stale data conditions  
**so that** I can maintain user trust by clearly signaling when the chart is not based on up-to-date or complete information.

## Acceptance Criteria

1. The pipeline can detect missing or incomplete input series from external providers (e.g., gaps in BTC price, LTH metrics, or both).
2. When any required series is incomplete for the current day or recent history window, the pipeline marks `dataQuality` as `partial` in `chart-data.json`.
3. When no new data is available for longer than the configured critical threshold (per architecture), the pipeline marks `dataQuality` as `stale`.
4. Unit tests cover:
   - Detection of partial data when one or more required series are missing recent points.
   - Detection of stale data when the last available point is older than the critical threshold.
5. Integration tests verify that running the pipeline with partial/stale fixtures produces a `chart-data.json` where:
   - `dataQuality` is set to `partial` or `stale` as appropriate.
   - `btcPrice` and `phaseScore` arrays remain internally consistent (aligned timestamps).
6. The story documents how `dataQuality` values map to user-facing behavior (e.g., Notion copy or chart banners) or references the relevant PRD/architecture sections.

## Tasks / Subtasks

- [ ] **Task 1 – Define required data completeness rules**  
  - [ ] Identify which input series are required for a valid phase score (e.g., BTC price, LTH SOPR-like, LTH MVRV-like).
  - [ ] Define what constitutes "partial" vs "stale" for the MVP (e.g., no data for the last N hours when Phase Score > 80 or < 20, or missing any required series for the last day).
  - [ ] Document these rules in the architecture (`Phase-Based Architectural Changes` and/or a new "Data Quality" subsection).

- [ ] **Task 2 – Implement data quality evaluation logic**  
  - [ ] Add a `data_quality` evaluation function (e.g., in `timing_terminal.scoring` or a dedicated `quality` module) that accepts the current `PhasePoint` series and returns `"complete"`, `"partial"`, or `"stale"`.
  - [ ] Ensure the evaluation logic uses the configured thresholds from `config.py` (e.g., max age in hours for fresh data).
  - [ ] Write unit tests covering all branches (complete, partial, stale) and edge cases (e.g., exactly at threshold, no data at all).

- [ ] **Task 3 – Extend fixtures to simulate partial and stale data**  
  - [ ] Add additional fixture sets under `pipeline/timing_terminal/data_providers/fixtures/` that represent:
    - A partial-data scenario (e.g., missing the most recent LTH series point).
    - A stale-data scenario (e.g., last available price/LTH data older than the critical threshold).
  - [ ] Document each fixture’s intent in comments or a short `README` in the fixtures directory.

- [ ] **Task 4 – Integrate quality evaluation into pipeline output**  
  - [ ] Update the pipeline’s `ChartData` creation flow so that `data_quality` is derived from the new evaluation function rather than hard-coded.
  - [ ] Adjust the CLI (and any relevant orchestration code) to ensure that partial/stale conditions are reflected in the printed summary.

- [ ] **Task 5 – Integration tests for partial and stale scenarios**  
  - [ ] Add integration tests that run the CLI against the new partial and stale fixtures and assert:
    - `dataQuality` is `"partial"` when some but not all required series are missing or truncated.
    - `dataQuality` is `"stale"` when data is older than the configured threshold.
  - [ ] Verify that `btcPrice` and `phaseScore` arrays remain aligned even when data is partial.

## Dev Notes

> This story builds directly on Story 1.1. It should reuse the existing `PhasePoint`, `ChartData`, and CLI structure while introducing clear, testable rules for `dataQuality` as described in the architecture (`External Providers`, `Phase-Based Architectural Changes`, and `Monitoring & Alerts`).
